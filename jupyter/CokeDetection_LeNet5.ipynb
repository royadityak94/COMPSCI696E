{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the required packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, Conv3D, Flatten, MaxPool2D, AveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.python.keras import metrics\n",
    "from keras import regularizers\n",
    "from keras.constraints import unit_norm\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.transform import rescale, resize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "MAX_CIFAR = 1000\n",
    "MAX_TRAIN, MAX_VAL, MAX_TEST = 6500, 3500, 2500 \n",
    "final_img_size = (96, 96, 3)\n",
    "MULTIPLIER = int(final_img_size[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Modules\n",
    "def get_random_data(data1, data2, low, high, max_samples=100, multiplier=1):\n",
    "    _, H1, W1, C1 = data1.shape\n",
    "    _, N = data2.shape\n",
    "    suff_data1 = np.empty((max_samples, H1*multiplier, W1*multiplier, C1))\n",
    "    suff_data2 = np.empty((max_samples, N))\n",
    "    shuffles = np.random.randint(low, high, max_samples)\n",
    "    for idx in range(shuffles.shape[0]):\n",
    "        suff_data1[idx] = rescale(data1[shuffles[idx], :, :, :], multiplier, anti_aliasing=False)\n",
    "        suff_data2[idx] = data2[shuffles[idx], :]\n",
    "    return suff_data1, suff_data2\n",
    "\n",
    "def load_formatted_image(input_dir, limit=None, img_size=final_img_size):\n",
    "    all_files = os.listdir(input_dir)\n",
    "    if limit is not None:\n",
    "        np.random.shuffle(all_files)\n",
    "        all_files = all_files[:limit]\n",
    "    img_arr = np.zeros((len(all_files), img_size[0], img_size[1], img_size[2]))\n",
    "    for idx in range(len(all_files)):\n",
    "        img_arr[idx] = resize(plt.imread(os.path.join(input_dir, all_files[idx])), img_size, anti_aliasing=False)\n",
    "        \n",
    "    return img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CIFAR dataset\n",
    "cifar = tf.keras.datasets.cifar10 \n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "X, y = np.concatenate((x_train, x_test), axis=0), np.concatenate((y_train, y_test), axis=0)\n",
    "res_X, _ = get_random_data(X, y, 0, X.shape[0], MAX_CIFAR, multiplier=MULTIPLIER)\n",
    "\n",
    "# Loading KAIST and Soda dataset\n",
    "kaist_arr = load_formatted_image('../data/KAIST/')\n",
    "soda_arr = load_formatted_image('../data/all_soda_bottles/')\n",
    "caltech_handpicked = load_formatted_image('../data/101_handpicked/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging KAIST and CIFAR dataset and creating training, test splits\n",
    "y_others = np.zeros(((kaist_arr.shape[0]+res_X.shape[0]), 1))\n",
    "y_soda = np.ones((soda_arr.shape[0], 1))\n",
    "\n",
    "pre_X, pre_y = np.concatenate((soda_arr, kaist_arr, res_X)), np.concatenate((y_soda, y_others))\n",
    "buff_X, buff_y = get_random_data(pre_X, pre_y, 0, pre_X.shape[0], pre_X.shape[0])\n",
    "\n",
    "# Weighing in the Caltech Object Dataset\n",
    "X = np.concatenate((buff_X[:(MAX_TRAIN+MAX_VAL+MAX_TEST+100)], caltech_handpicked))\n",
    "y = np.concatenate((buff_y[:X.shape[0]], np.zeros((caltech_handpicked.shape[0], 1))))\n",
    "\n",
    "X_train, y_train = X[:MAX_TRAIN], y[:MAX_TRAIN]\n",
    "X_val, y_val = X[MAX_TRAIN:MAX_TRAIN+MAX_VAL], y[MAX_TRAIN:MAX_TRAIN+MAX_VAL]\n",
    "X_test, y_test = X[MAX_TRAIN+MAX_VAL:MAX_TRAIN+MAX_VAL+MAX_TEST], y[MAX_TRAIN+MAX_VAL:MAX_TRAIN+MAX_VAL+MAX_TEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "activation_func = 'selu'\n",
    "weight_decay = 1e-5\n",
    "curr_optimizer = 'Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed variant of the LeNet-5 architecture\n",
    "model = Sequential()\n",
    "\n",
    "#Instantiating Layer 1\n",
    "model.add(Conv2D(6, kernel_size=(3, 3), strides=(1, 1), activation=activation_func, padding='valid', \n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2), name='pool1'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# #Instantiating Layer 2\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), strides=(1, 1), activation=activation_func, padding='valid', \n",
    "                kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2), name='pool2'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# #Instantiating Layer 3\n",
    "model.add(Conv2D(120, kernel_size=(3, 3), strides=(1, 1), activation=activation_func, padding='valid', \n",
    "                kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Flatten())\n",
    "\n",
    "#Instantiating Layer 4\n",
    "model.add(Dense(84, activation=activation_func)) \n",
    "\n",
    "#Output Layer\n",
    "model.add(Dense(1, activation='sigmoid'))    \n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=curr_optimizer, validation_data=(X_val, y_val),\n",
    "                  metrics=[\"accuracy\", \"mae\", \"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples\n",
      "Epoch 1/12\n",
      "6500/6500 [==============================] - 24s 4ms/sample - loss: 0.9515 - accuracy: 0.9126 - mae: 0.0873 - mse: 0.0674\n",
      "Epoch 2/12\n",
      "6500/6500 [==============================] - 21s 3ms/sample - loss: 0.0232 - accuracy: 0.9943 - mae: 0.0083 - mse: 0.0041\n",
      "Epoch 3/12\n",
      "6500/6500 [==============================] - 22s 3ms/sample - loss: 0.0038 - accuracy: 0.9995 - mae: 0.0014 - mse: 5.4578e-04\n",
      "Epoch 4/12\n",
      "6500/6500 [==============================] - 21s 3ms/sample - loss: 0.0017 - accuracy: 0.9998 - mae: 4.4920e-04 - mse: 2.0691e-04\n",
      "Epoch 5/12\n",
      "6500/6500 [==============================] - 21s 3ms/sample - loss: 6.4570e-04 - accuracy: 1.0000 - mae: 2.1322e-04 - mse: 3.0125e-05\n",
      "Epoch 6/12\n",
      "6500/6500 [==============================] - 21s 3ms/sample - loss: 6.1733e-04 - accuracy: 1.0000 - mae: 1.8907e-04 - mse: 2.3465e-05\n",
      "Epoch 7/12\n",
      "6500/6500 [==============================] - 22s 3ms/sample - loss: 5.2008e-04 - accuracy: 1.0000 - mae: 1.0263e-04 - mse: 4.9092e-06\n",
      "Epoch 8/12\n",
      "6500/6500 [==============================] - 22s 3ms/sample - loss: 5.0421e-04 - accuracy: 1.0000 - mae: 8.7536e-05 - mse: 3.4630e-06\n",
      "Epoch 9/12\n",
      "6500/6500 [==============================] - 22s 3ms/sample - loss: 4.7821e-04 - accuracy: 1.0000 - mae: 6.2428e-05 - mse: 1.7948e-06\n",
      "Epoch 10/12\n",
      "6500/6500 [==============================] - 22s 3ms/sample - loss: 4.6394e-04 - accuracy: 1.0000 - mae: 4.8588e-05 - mse: 9.8446e-07\n",
      "Epoch 11/12\n",
      "6500/6500 [==============================] - 21s 3ms/sample - loss: 4.5809e-04 - accuracy: 1.0000 - mae: 4.2871e-05 - mse: 7.5404e-07\n",
      "Epoch 12/12\n",
      "6500/6500 [==============================] - 22s 3ms/sample - loss: 4.5296e-04 - accuracy: 1.0000 - mae: 3.7868e-05 - mse: 5.4407e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7f044e9790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=12, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=100)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coca_cola_can.jpg  coke_can.jpg  coke_can2.jpg  coke_can3.jpg  coke_can4.jpg  milk_room.jpg  vodka.jpg  final_img_size.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and evaluate Test set \n",
    "# im = resize(plt.imread('../data/manual_coke_set/vodka.jpg'), final_img_size, anti_aliasing=False)\n",
    "# plt.imshow(im)\n",
    "# imz = im[np.newaxis]\n",
    "\n",
    "# y_pred = model.predict(imz)\n",
    "# y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
